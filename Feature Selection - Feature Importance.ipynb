{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e46f2b5-8872-45a8-a8bb-458567751f19",
   "metadata": {},
   "source": [
    "## Feature Selection Using Feature Importance\n",
    "\n",
    "Feature selection is the process of identifying and selecting the most relevant features from a dataset to improve model performance and interpretability. Feature importance provides a systematic way to select features by evaluating their contribution to the predictive model.\n",
    "\n",
    "### Key Steps in Feature Selection Using Feature Importance\n",
    "\n",
    "1. **Train a Model:**\n",
    "   - Use a model that provides feature importance scores, such as tree-based models like Random Forest or Gradient Boosting.\n",
    "\n",
    "2. **Compute Feature Importances:**\n",
    "   - Extract importance scores for each feature.\n",
    "\n",
    "3. **Rank Features:**\n",
    "   - Rank the features based on their importance scores.\n",
    "\n",
    "4. **Select Top Features:**\n",
    "   - Choose a subset of features based on their importance.\n",
    "\n",
    "5. **Train a Model with Selected Features:**\n",
    "   - Retrain the model using only the selected features and evaluate its performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "574f37fc-ddeb-4a4a-919f-76ab3f7777bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importances:\n",
      "             Feature  Importance\n",
      "3   petal width (cm)    0.433982\n",
      "2  petal length (cm)    0.417308\n",
      "0  sepal length (cm)    0.104105\n",
      "1   sepal width (cm)    0.044605\n",
      "\n",
      "Selected Features:\n",
      "3     petal width (cm)\n",
      "2    petal length (cm)\n",
      "0    sepal length (cm)\n",
      "Name: Feature, dtype: object\n",
      "\n",
      "Accuracy with Selected Features: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "# Load sample dataset\n",
    "data = load_iris()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = data.target\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize and train Random Forest model\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Extract feature importances\n",
    "importances = rf_model.feature_importances_\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"Feature Importances:\")\n",
    "print(feature_importance_df)\n",
    "\n",
    "# Select top features based on importance threshold\n",
    "importance_threshold = 0.1  # Define threshold\n",
    "selected_features = feature_importance_df[feature_importance_df['Importance'] >= importance_threshold]['Feature']\n",
    "\n",
    "print(\"\\nSelected Features:\")\n",
    "print(selected_features)\n",
    "\n",
    "# Train a new model using only selected features\n",
    "X_train_selected = X_train[selected_features]\n",
    "X_test_selected = X_test[selected_features]\n",
    "\n",
    "rf_model_selected = RandomForestClassifier(random_state=42)\n",
    "rf_model_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Evaluate the model with selected features\n",
    "y_pred = rf_model_selected.predict(X_test_selected)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nAccuracy with Selected Features: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167ed677-c839-4598-a7d9-55cc0f1ded02",
   "metadata": {},
   "source": [
    "### Feature Importances\n",
    "| Feature             | Importance |\n",
    "|---------------------|------------|\n",
    "| petal width (cm)    | 0.433982   |\n",
    "| petal length (cm)   | 0.417308   |\n",
    "| sepal length (cm)   | 0.104105   |\n",
    "| sepal width (cm)    | 0.044605   |\n",
    "\n",
    "---\n",
    "\n",
    "### Selected Features\n",
    "| Selected Features     |\n",
    "|------------------------|\n",
    "| petal width (cm)       |\n",
    "| petal length (cm)      |\n",
    "| sepal length (cm)      |\n",
    "\n",
    "---\n",
    "\n",
    "### Accuracy\n",
    "| Metric                   | Value |\n",
    "|--------------------------|-------|\n",
    "| Accuracy with Selected Features | 1.0   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e5656e-8131-4aea-bd2e-cb6491f0bd96",
   "metadata": {},
   "source": [
    "### Explanation of the Code\n",
    "\n",
    "**Load Dataset:**\n",
    "- The Iris dataset is used, which has 4 features (sepal length, sepal width, petal length, petal width).\n",
    "\n",
    "**Train Random Forest Model:**\n",
    "- A Random Forest model is trained on the dataset. This model provides `feature_importances_` scores.\n",
    "\n",
    "**Compute Feature Importances:**\n",
    "- The feature importance scores are extracted and sorted to rank the features.\n",
    "\n",
    "**Feature Selection:**\n",
    "- Features with an importance score above a specified threshold (e.g., 0.1) are selected.\n",
    "\n",
    "**Retrain Model:**\n",
    "- A new Random Forest model is trained using only the selected features.\n",
    "\n",
    "**Evaluate Model:**\n",
    "- The accuracy of the model using the selected features is calculated to ensure performance is retained or improved.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a24e92-6a75-470b-9d45-0593a5840df0",
   "metadata": {},
   "source": [
    "### Benefits of Feature Selection Using Feature Importance\n",
    "\n",
    "1. **Improved Model Performance**  \n",
    "   Reducing irrelevant or redundant features can enhance the model's accuracy and generalization.\n",
    "\n",
    "2. **Reduced Complexity**  \n",
    "   Simplifying the feature set reduces training time and computational cost.\n",
    "\n",
    "3. **Better Interpretability**  \n",
    "   Identifying important features helps in understanding the factors driving predictions.\n",
    "\n",
    "4. **Noise Reduction**  \n",
    "   Eliminating less important features can reduce noise in the data and improve robustness.\n",
    "\n",
    "---\n",
    "\n",
    "This method can be applied with other algorithms like Gradient Boosting (e.g., XGBoost, LightGBM) or with model-agnostic techniques like Permutation Importance or SHAP values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179db1bc-0911-411a-819c-1d7c838f99c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
